# Redact Project - Claude Instructions

## Project Overview
**AWS Document Scrubbing System** - Automatically detects and removes client names/logos from uploaded documents using serverless AWS infrastructure.

## Current Status
- âœ… **Enterprise Production Ready**: All features implemented and tested
- âœ… **Cost-Optimized**: Reduced from $30-40 to $0-5/month  
- âœ… **Multi-Channel Processing**: API Gateway + Direct S3 upload
- âœ… **Comprehensive Testing**: 80%+ coverage with CI/CD pipeline
- âœ… **REST API Enabled**: Upload, status checking, health monitoring
- âœ… **Batch Processing**: Multiple file handling with timeout controls
- ğŸ“‹ **Enterprise Documentation**: Complete operational guides

## Quick Commands

### Deploy Complete System (Recommended)
```bash
cd redact-terraform
./deploy-improvements.sh
```

### Deploy Infrastructure Only
```bash
cd redact-terraform
terraform apply -auto-approve
```

### Configure Redaction Rules
```bash
# Create config file
cat > config.json << 'EOF'
{
  "replacements": [
    {"find": "REPLACE_CLIENT_NAME", "replace": "Company X"},
    {"find": "ACME Corporation", "replace": "[REDACTED]"},
    {"find": "Confidential", "replace": "[REDACTED]"}
  ],
  "case_sensitive": false
}
EOF

# Upload config to S3
aws s3 cp config.json s3://redact-config-32a4ee51/
```

### Test Document Processing

#### Option A: REST API (Recommended)
```bash
# Get API URL
API_URL=https://101pi5aiv5.execute-api.us-east-1.amazonaws.com/production

# Test health
curl -X GET "$API_URL/health"

# Upload document
echo "Confidential data from ACME Corporation" > test.txt
curl -X POST "$API_URL/documents/upload" \
  -H "Content-Type: application/json" \
  -d '{"filename": "test.txt", "content": "'$(base64 -w0 test.txt)'"}'

# Check status (use document_id from upload response)
curl -X GET "$API_URL/documents/status/{document_id}"
```

#### Option B: Direct S3 Upload
```bash
# Upload test document
echo "Confidential data from ACME Corporation" > test.txt
aws s3 cp test.txt s3://redact-input-documents-32a4ee51/

# Check processed results (wait 30 seconds)
aws s3 ls s3://redact-processed-documents-32a4ee51/processed/
```

### Monitor Processing
```bash
# View Lambda logs
aws logs tail /aws/lambda/document-scrubbing-processor --follow

# Check for errors
aws logs filter-log-events --log-group-name /aws/lambda/document-scrubbing-processor --filter-pattern "ERROR"

# Check DLQ for failed messages
aws sqs get-queue-attributes \
  --queue-url $(aws sqs get-queue-url --queue-name document-scrubbing-dlq --query QueueUrl --output text) \
  --attribute-names ApproximateNumberOfMessages
```

### Run Tests
```bash
# Run comprehensive test suite
./run-tests.sh

# Run specific tests
python -m pytest tests/test_lambda_function.py -v
python -m pytest tests/test_integration.py -v
```

### Get Resource Info
```bash
terraform output
```

## Live AWS Resources
```
Input Bucket:       redact-input-documents-32a4ee51
Processed Bucket:   redact-processed-documents-32a4ee51  
Quarantine Bucket:  redact-quarantine-documents-32a4ee51
Config Bucket:      redact-config-32a4ee51
Lambda Functions:   document-scrubbing-processor, redact-api-handler
API Gateway:        https://101pi5aiv5.execute-api.us-east-1.amazonaws.com/production
Dead Letter Queue:  document-scrubbing-dlq
```

## Architecture Summary
```
External Users â†’ API Gateway â†’ Lambda (Batch) â†’ S3 Storage
                     â†“              â†“               â†“
               CORS/Auth     DLQ + Retry    Lifecycle Policies
                     â†“              â†“               â†“
               CloudWatch â† Monitoring â†’ Budget Alerts

Alternative: Direct S3 Upload â†’ Event Trigger â†’ Lambda Processing
                     (AES256 encrypted)    (Multi-format + validation)
```

## File Structure
```
redact-terraform/
â”œâ”€â”€ main.tf                    # Core infrastructure (S3, lifecycle policies)
â”œâ”€â”€ lambda.tf                  # Document processing Lambda
â”œâ”€â”€ api-gateway.tf             # REST API Gateway and API Lambda
â”œâ”€â”€ variables.tf               # Configuration parameters
â”œâ”€â”€ outputs.tf                 # Resource outputs and API endpoints
â”œâ”€â”€ lambda_code/               # Document processing logic
â”‚   â”œâ”€â”€ lambda_function.py     # Enhanced with batch processing
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ api_code/                  # API Gateway handlers
â”‚   â””â”€â”€ api_handler.py         # Upload, status, health endpoints
â”œâ”€â”€ tests/                     # Comprehensive test suite
â”‚   â”œâ”€â”€ test_lambda_function.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ .github/workflows/         # CI/CD automation
â”‚   â”œâ”€â”€ ci-cd.yml             # Main pipeline
â”‚   â””â”€â”€ pr-validation.yml     # PR validation
â”œâ”€â”€ monitoring-dashboard.json  # CloudWatch configuration
â”œâ”€â”€ budget-alert.json         # Cost control
â”œâ”€â”€ run-tests.sh              # Test automation
â”œâ”€â”€ deploy-improvements.sh     # Deployment automation
â”œâ”€â”€ README.md                 # Project overview
â”œâ”€â”€ DEPLOYMENT.md             # Setup instructions
â”œâ”€â”€ STATUS.md                 # Current progress
â”œâ”€â”€ IMPROVEMENTS.md           # Enhancement summary
â”œâ”€â”€ NEXT_STEPS.md            # Future roadmap
â””â”€â”€ CLAUDE.md                # This file
```

## Security Features
- ğŸ” **AES256 Encryption**: All data encrypted at rest
- ğŸš« **Public Access Blocked**: All S3 buckets private
- ğŸ”’ **IAM Least Privilege**: Minimal required permissions
- ğŸ›¡ï¸ **Input Validation**: File size/type restrictions, sanitization
- ğŸ”„ **Retry Logic**: Exponential backoff with DLQ for failures
- ğŸ” **Configuration Validation**: JSON schema checking with fallback
- ğŸŒ **API Authentication**: IAM-based security for REST endpoints
- ğŸ“Š **Tagged Resources**: Project=redact for cost tracking
- ğŸ’° **Cost-Optimized**: No VPC/KMS charges, within free tier

## What It Does
1. **Upload**: Documents via REST API or direct S3 upload (.txt, .pdf, .docx, .xlsx)
2. **Validate**: File size (50MB limit), type checking, and input sanitization
3. **Process**: Batch processing with retry logic and timeout controls
4. **Config**: Lambda reads redaction rules from config.json in config bucket
5. **Redact**: Replace configured text patterns and remove images from documents
6. **Output**: Clean documents â†’ processed bucket, errors â†’ quarantine bucket
7. **Monitor**: Real-time status via API endpoints and CloudWatch dashboard

## Configuration System
- **Flexible Rules**: Define find/replace patterns in config.json
- **No Code Changes**: Update redaction rules without redeploying
- **Multi-Format**: Supports text, PDF, DOCX, and XLSX files
- **Image Removal**: Automatically strips images from documents
- **Case Control**: Configure case-sensitive or case-insensitive matching

## Development Context for Claude

### Working with this project:
- Infrastructure is **Terraform-managed** - always use `terraform` commands for changes
- All resources are **tagged** with `Project = "redact"` for billing
- Lambda function uses **Python 3.11** runtime
- Processing logic is in `lambda_code/lambda_function.py`

### Common Tasks:
- **Update redaction rules**: Modify and upload `config.json` to config bucket
- **Add new file types**: Update `lambda_function.py` processing logic
- **Scale resources**: Adjust variables in `variables.tf`
- **Monitor costs**: Filter AWS billing by `Project = "redact"` tag

### Config File Format:
```json
{
  "replacements": [
    {"find": "CLIENT_NAME", "replace": "Company X"},
    {"find": "John Smith", "replace": "[NAME REDACTED]"},
    {"find": "555-123-4567", "replace": "[PHONE REDACTED]"}
  ],
  "case_sensitive": false
}
```

### Testing Strategy:
- Upload documents with known client names (ACME Corporation, TechnoSoft LLC)
- Verify redaction by downloading processed files
- Check quarantine bucket for sensitive content
- Monitor CloudWatch logs for processing status

### Security Notes:
- All data **encrypted at rest** with AWS-managed AES256
- **Public access blocked** on all S3 buckets
- Lambda has **minimal IAM permissions** - only what's needed
- **CloudWatch logging** for audit trails

### Cost Optimization:
- Current cost: **$0-5/month** for light usage (within free tier)
- Previous cost was $30-40/month before optimization
- Removed VPC infrastructure (saved ~$22/month)
- Removed customer KMS key (saved $1/month)
- S3 lifecycle policies automatically reduce storage costs

### Current Capabilities (Enterprise-Ready):
1. âœ… **Multi-format Processing**: TXT, PDF, DOCX, XLSX with image removal
2. âœ… **REST API Gateway**: Upload, status checking, health monitoring
3. âœ… **Batch Processing**: Multiple files with timeout controls
4. âœ… **Comprehensive Testing**: 80%+ coverage with CI/CD pipeline
5. âœ… **Security Hardened**: Input validation, retry logic, DLQ monitoring
6. âœ… **Production Monitoring**: CloudWatch dashboard and budget alerts
7. âœ… **Configurable Rules**: S3-based redaction config with validation
8. âœ… **Cost Optimized**: $0-5/month (down from $30-40/month)

### Emergency Procedures:
- **Stop processing**: Disable S3 bucket notifications
- **View logs**: CloudWatch Logs for debugging
- **Rollback**: `terraform destroy` removes all resources
- **Backup**: All code in git, infrastructure as code

### Key Files to Modify:
- `config.json` in S3 - Redaction rules (no deployment needed)
- `lambda_function.py` - Processing logic
- `variables.tf` - Configuration changes
- `main.tf` - Infrastructure changes
- Documentation files for updates

This project demonstrates a production-ready, enterprise-grade serverless document processing system with comprehensive testing, monitoring, REST API integration, and cost optimization. The system is fully operational and ready for production workloads.